{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba8ba6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üï∏Ô∏è  Starting Universal Crawler (v3)...\n",
      "üìñ Reading Page 1...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 2...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 3...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 4...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 5...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 6...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 7...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 8...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 9...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 10...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 11...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "‚úÖ Reached end of the list (No 'Next page' link found).\n",
      "\n",
      "üéâ Crawler Finished! Found 0 total pages.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "# CONFIGURATION\n",
    "BASE_URL = \"https://wiki.metakgp.org\"\n",
    "SEED_URL = \"https://wiki.metakgp.org/w/Special:AllPages\"\n",
    "\n",
    "# Namespaces to strictly ignore\n",
    "IGNORED_NAMESPACES = [\n",
    "    \"Special:\", \"Talk:\", \"User:\", \"User_talk:\", \"Metakgp:\", \n",
    "    \"Metakgp_talk:\", \"File:\", \"File_talk:\", \"MediaWiki:\", \n",
    "    \"Template:\", \"Help:\", \"Category:\", \"Category_talk:\"\n",
    "]\n",
    "\n",
    "def crawl_all_urls():\n",
    "    print(\"üï∏Ô∏è  Starting Universal Crawler (v3)...\")\n",
    "    current_url = SEED_URL\n",
    "    all_links = []\n",
    "    page_counter = 1\n",
    "\n",
    "    while current_url:\n",
    "        print(f\"üìñ Reading Page {page_counter}...\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå Failed to load: {current_url}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # --- 1. REMOVE NOISE (Sidebar & Footer) ---\n",
    "            # We destroy the sidebar and footer from the soup object before searching.\n",
    "            # This ensures we don't accidentally grab \"Main Page\" or \"About\" links.\n",
    "            for garbage in soup.find_all(class_=['mw-panel', 'vector-menu-portal', 'footer', 'mw-footer']):\n",
    "                garbage.decompose()\n",
    "            for garbage in soup.find_all(id=['mw-panel', 'footer', 'mw-navigation']):\n",
    "                garbage.decompose()\n",
    "\n",
    "            # --- 2. FIND ALL REMAINING LINKS ---\n",
    "            # Now the only links left should be in the content area.\n",
    "            links = soup.find_all('a', href=True)\n",
    "            found_on_this_page = 0\n",
    "            \n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                full_url = urljoin(BASE_URL, href)\n",
    "                \n",
    "                # --- 3. FILTER LOGIC ---\n",
    "                # A. Must be a Wiki link\n",
    "                if \"/wiki/\" not in href:\n",
    "                    continue\n",
    "                \n",
    "                # B. Must NOT be an Admin/System page\n",
    "                is_banned = False\n",
    "                for ns in IGNORED_NAMESPACES:\n",
    "                    if ns in href:\n",
    "                        is_banned = True\n",
    "                        break\n",
    "                \n",
    "                if is_banned:\n",
    "                    continue\n",
    "\n",
    "                # C. Must NOT be the \"Next Page\" pagination link\n",
    "                if \"Next page\" in link.text or \"Previous page\" in link.text:\n",
    "                    continue\n",
    "\n",
    "                # If we passed all checks, it's a valid article!\n",
    "                all_links.append(full_url)\n",
    "                found_on_this_page += 1\n",
    "\n",
    "            print(f\"   -> Found {found_on_this_page} valid links on this page.\")\n",
    "            \n",
    "            # --- DEBUG: If 0 found, print what we DID see to help debug ---\n",
    "            if found_on_this_page == 0:\n",
    "                print(\"   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\")\n",
    "                for l in links[:5]:\n",
    "                    print(f\"      - Text: '{l.text}' | Href: '{l['href']}'\")\n",
    "\n",
    "            # --- 4. PAGINATION ---\n",
    "            # We look for the \"Next page\" link specifically.\n",
    "            next_link = None\n",
    "            # Re-fetch all links including navigation (since we decomposed them earlier, \n",
    "            # we might need to check if we deleted the nav. \n",
    "            # Actually, the 'Next' link is usually in the content body or top/bottom of list.\n",
    "            # If we decomposed 'mw-navigation', we might have killed it.\n",
    "            # Let's check the UN-MODIFIED text for pagination link.\n",
    "            \n",
    "            # Strategy: Search the raw text for the 'Next page' link pattern if soup failed\n",
    "            pagination_soup = BeautifulSoup(response.text, 'html.parser') # Fresh soup\n",
    "            nav_links = pagination_soup.find_all(\"a\", href=True)\n",
    "            \n",
    "            for link in nav_links:\n",
    "                if \"Next page\" in link.text:\n",
    "                    next_link = urljoin(BASE_URL, link['href'])\n",
    "                    break\n",
    "            \n",
    "            if next_link:\n",
    "                current_url = next_link\n",
    "                page_counter += 1\n",
    "                time.sleep(0.5)\n",
    "            else:\n",
    "                print(\"‚úÖ Reached end of the list (No 'Next page' link found).\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL ERROR: {e}\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nüéâ Crawler Finished! Found {len(all_links)} total pages.\")\n",
    "    return all_links\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_list = crawl_all_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ee1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
