{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a82446c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üï∏Ô∏è  Starting Universal Crawler (v3)...\n",
      "üìñ Reading Page 1...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 2...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 3...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 4...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 5...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 6...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 7...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 8...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 9...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 10...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "üìñ Reading Page 11...\n",
      "   -> Found 0 valid links on this page.\n",
      "   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\n",
      "      - Text: 'Jump to content' | Href: '#bodyContent'\n",
      "      - Text: 'Main page' | Href: '/w/Main_Page'\n",
      "      - Text: 'Yellow pages' | Href: '/w/Yellow_pages'\n",
      "      - Text: 'Recent changes' | Href: '/w/Special:RecentChanges'\n",
      "      - Text: 'Random article' | Href: '/w/Special:Random'\n",
      "‚úÖ Reached end of the list (No 'Next page' link found).\n",
      "\n",
      "üéâ Crawler Finished! Found 0 total pages.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "# CONFIGURATION\n",
    "BASE_URL = \"https://wiki.metakgp.org\"\n",
    "SEED_URL = \"https://wiki.metakgp.org/w/Special:AllPages\"\n",
    "\n",
    "# Namespaces to strictly ignore\n",
    "IGNORED_NAMESPACES = [\n",
    "    \"Special:\", \"Talk:\", \"User:\", \"User_talk:\", \"Metakgp:\", \n",
    "    \"Metakgp_talk:\", \"File:\", \"File_talk:\", \"MediaWiki:\", \n",
    "    \"Template:\", \"Help:\", \"Category:\", \"Category_talk:\"\n",
    "]\n",
    "\n",
    "def crawl_all_urls():\n",
    "    print(\"üï∏Ô∏è  Starting Universal Crawler (v3)...\")\n",
    "    current_url = SEED_URL\n",
    "    all_links = []\n",
    "    page_counter = 1\n",
    "\n",
    "    while current_url:\n",
    "        print(f\"üìñ Reading Page {page_counter}...\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå Failed to load: {current_url}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # --- 1. REMOVE NOISE (Sidebar & Footer) ---\n",
    "            # We destroy the sidebar and footer from the soup object before searching.\n",
    "            # This ensures we don't accidentally grab \"Main Page\" or \"About\" links.\n",
    "            for garbage in soup.find_all(class_=['mw-panel', 'vector-menu-portal', 'footer', 'mw-footer']):\n",
    "                garbage.decompose()\n",
    "            for garbage in soup.find_all(id=['mw-panel', 'footer', 'mw-navigation']):\n",
    "                garbage.decompose()\n",
    "\n",
    "            # --- 2. FIND ALL REMAINING LINKS ---\n",
    "            # Now the only links left should be in the content area.\n",
    "            links = soup.find_all('a', href=True)\n",
    "            found_on_this_page = 0\n",
    "            \n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                full_url = urljoin(BASE_URL, href)\n",
    "                \n",
    "                # --- 3. FILTER LOGIC ---\n",
    "                # A. Must be a Wiki link\n",
    "                if \"/wiki/\" not in href:\n",
    "                    continue\n",
    "                \n",
    "                # B. Must NOT be an Admin/System page\n",
    "                is_banned = False\n",
    "                for ns in IGNORED_NAMESPACES:\n",
    "                    if ns in href:\n",
    "                        is_banned = True\n",
    "                        break\n",
    "                \n",
    "                if is_banned:\n",
    "                    continue\n",
    "\n",
    "                # C. Must NOT be the \"Next Page\" pagination link\n",
    "                if \"Next page\" in link.text or \"Previous page\" in link.text:\n",
    "                    continue\n",
    "\n",
    "                # If we passed all checks, it's a valid article!\n",
    "                all_links.append(full_url)\n",
    "                found_on_this_page += 1\n",
    "\n",
    "            print(f\"   -> Found {found_on_this_page} valid links on this page.\")\n",
    "            \n",
    "            # --- DEBUG: If 0 found, print what we DID see to help debug ---\n",
    "            if found_on_this_page == 0:\n",
    "                print(\"   ‚ö†Ô∏è DEBUG: Here are the first 5 raw links I saw (and rejected):\")\n",
    "                for l in links[:5]:\n",
    "                    print(f\"      - Text: '{l.text}' | Href: '{l['href']}'\")\n",
    "\n",
    "            # --- 4. PAGINATION ---\n",
    "            # We look for the \"Next page\" link specifically.\n",
    "            next_link = None\n",
    "            # Re-fetch all links including navigation (since we decomposed them earlier, \n",
    "            # we might need to check if we deleted the nav. \n",
    "            # Actually, the 'Next' link is usually in the content body or top/bottom of list.\n",
    "            # If we decomposed 'mw-navigation', we might have killed it.\n",
    "            # Let's check the UN-MODIFIED text for pagination link.\n",
    "            \n",
    "            # Strategy: Search the raw text for the 'Next page' link pattern if soup failed\n",
    "            pagination_soup = BeautifulSoup(response.text, 'html.parser') # Fresh soup\n",
    "            nav_links = pagination_soup.find_all(\"a\", href=True)\n",
    "            \n",
    "            for link in nav_links:\n",
    "                if \"Next page\" in link.text:\n",
    "                    next_link = urljoin(BASE_URL, link['href'])\n",
    "                    break\n",
    "            \n",
    "            if next_link:\n",
    "                current_url = next_link\n",
    "                page_counter += 1\n",
    "                time.sleep(0.5)\n",
    "            else:\n",
    "                print(\"‚úÖ Reached end of the list (No 'Next page' link found).\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL ERROR: {e}\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nüéâ Crawler Finished! Found {len(all_links)} total pages.\")\n",
    "    return all_links\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_list = crawl_all_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "390d3553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\Gaurav Gupta\\AppData\\Local\\Temp\\ipykernel_21696\\3903598030.py:8: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  INPUT_DIR = \"C:\\programming\\prg\\Devsoc-hackathon\\scraped_data\"  # Directory containing your batch_*.json files\n",
      "c:\\programming\\newenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Gaurav Gupta\\AppData\\Local\\Temp\\ipykernel_21696\\3903598030.py:8: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  INPUT_DIR = \"C:\\programming\\prg\\Devsoc-hackathon\\scraped_data\"  # Directory containing your batch_*.json files\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# CONFIGURATION\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\programming\\newenv\\Lib\\site-packages\\langchain_text_splitters\\__init__.py:8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"Text Splitters are classes for splitting text.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03m!!! note\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m    `MarkdownHeaderTextSplitter` and `HTMLHeaderTextSplitter` do not derive from\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    `TextSplitter`.\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_text_splitters\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     Language,\n\u001b[32m     10\u001b[39m     TextSplitter,\n\u001b[32m     11\u001b[39m     Tokenizer,\n\u001b[32m     12\u001b[39m     TokenTextSplitter,\n\u001b[32m     13\u001b[39m     split_text_on_tokens,\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_text_splitters\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcharacter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     CharacterTextSplitter,\n\u001b[32m     17\u001b[39m     RecursiveCharacterTextSplitter,\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_text_splitters\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhtml\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     ElementType,\n\u001b[32m     21\u001b[39m     HTMLHeaderTextSplitter,\n\u001b[32m     22\u001b[39m     HTMLSectionSplitter,\n\u001b[32m     23\u001b[39m     HTMLSemanticPreservingSplitter,\n\u001b[32m     24\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\programming\\newenv\\Lib\\site-packages\\langchain_text_splitters\\base.py:33\u001b[39m\n\u001b[32m     30\u001b[39m     _HAS_TIKTOKEN = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenization_utils_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizerBase\n\u001b[32m     35\u001b[39m     _HAS_TRANSFORMERS = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\programming\\newenv\\Lib\\site-packages\\transformers\\__init__.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     30\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     is_pretty_midi_available,\n\u001b[32m     37\u001b[39m )\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\programming\\newenv\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\programming\\newenv\\Lib\\site-packages\\transformers\\utils\\__init__.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_docstring\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     ClassAttrs,\n\u001b[32m     26\u001b[39m     ClassDocstring,\n\u001b[32m     27\u001b[39m     ImageProcessorArgs,\n\u001b[32m     28\u001b[39m     ModelArgs,\n\u001b[32m     29\u001b[39m     ModelOutputArgs,\n\u001b[32m     30\u001b[39m     auto_class_docstring,\n\u001b[32m     31\u001b[39m     auto_docstring,\n\u001b[32m     32\u001b[39m     get_args_doc_from_source,\n\u001b[32m     33\u001b[39m     parse_docstring,\n\u001b[32m     34\u001b[39m     set_min_indent,\n\u001b[32m     35\u001b[39m )\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\programming\\newenv\\Lib\\site-packages\\transformers\\utils\\auto_docstring.py:30\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mregex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     MODELS_TO_PIPELINE,\n\u001b[32m     26\u001b[39m     PIPELINE_TASKS_TO_SAMPLE_DOCSTRINGS,\n\u001b[32m     27\u001b[39m     PT_SAMPLE_DOCSTRINGS,\n\u001b[32m     28\u001b[39m     _prepare_output_docstrings,\n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelOutput\n\u001b[32m     33\u001b[39m PATH_TO_TRANSFORMERS = Path(\u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m).resolve() / \u001b[33m\"\u001b[39m\u001b[33mtransformers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m AUTODOC_FILES = [\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfiguration_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodeling_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature_extractor_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     44\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\programming\\newenv\\Lib\\site-packages\\transformers\\utils\\generic.py:51\u001b[39m\n\u001b[32m     47\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# required for @can_return_tuple decorator to work with torchdynamo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_debugging_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_addition_debugger_context\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# vendored from distutils.util\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\programming\\newenv\\Lib\\site-packages\\torch\\__init__.py:2099\u001b[39m\n\u001b[32m   2095\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnable to find torch_shm_manager at \u001b[39m\u001b[33m\"\u001b[39m + path)\n\u001b[32m   2096\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m path.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2099\u001b[39m \u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_initExtension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_manager_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2101\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m _manager_path\n\u001b[32m   2103\u001b[39m \u001b[38;5;66;03m# Appease the type checker: it can't deal with direct setting of globals().\u001b[39;00m\n\u001b[32m   2104\u001b[39m \u001b[38;5;66;03m# Note that we will see \"too many\" functions when reexporting this way; there\u001b[39;00m\n\u001b[32m   2105\u001b[39m \u001b[38;5;66;03m# is not a good way to fix this problem.  Perhaps, try to redesign VariableFunctions\u001b[39;00m\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# so that this import is good enough\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\programming\\newenv\\Lib\\site-packages\\torch\\cuda\\__init__.py:356\u001b[39m\n\u001b[32m    351\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    352\u001b[39m                 \u001b[38;5;66;03m# Don't store the actual traceback to avoid memory cycle\u001b[39;00m\n\u001b[32m    353\u001b[39m                 _queued_calls.append((\u001b[38;5;28mcallable\u001b[39m, traceback.format_stack()))\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_check_capability\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m _lazy_call(_check_cubins)\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDeferredCudaCallError\u001b[39;00m(\u001b[38;5;167;01mException\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\programming\\newenv\\Lib\\site-packages\\torch\\cuda\\__init__.py:353\u001b[39m, in \u001b[36m_lazy_call\u001b[39m\u001b[34m(callable, **kwargs)\u001b[39m\n\u001b[32m    350\u001b[39m     _lazy_seed_tracker.queue_seed(\u001b[38;5;28mcallable\u001b[39m, traceback.format_stack())\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    352\u001b[39m     \u001b[38;5;66;03m# Don't store the actual traceback to avoid memory cycle\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     _queued_calls.append((\u001b[38;5;28mcallable\u001b[39m, \u001b[43mtraceback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\programming\\python3.12.10\\Lib\\traceback.py:218\u001b[39m, in \u001b[36mformat_stack\u001b[39m\u001b[34m(f, limit)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    217\u001b[39m     f = sys._getframe().f_back\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m format_list(\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\programming\\python3.12.10\\Lib\\traceback.py:232\u001b[39m, in \u001b[36mextract_stack\u001b[39m\u001b[34m(f, limit)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    231\u001b[39m     f = sys._getframe().f_back\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m stack = \u001b[43mStackSummary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m stack.reverse()\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\programming\\python3.12.10\\Lib\\traceback.py:395\u001b[39m, in \u001b[36mStackSummary.extract\u001b[39m\u001b[34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[39m\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f, lineno \u001b[38;5;129;01min\u001b[39;00m frame_gen:\n\u001b[32m    393\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m f, (lineno, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_extract_from_extended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlookup_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapture_locals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapture_locals\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\programming\\python3.12.10\\Lib\\traceback.py:438\u001b[39m, in \u001b[36mStackSummary._extract_from_extended_frame_gen\u001b[39m\u001b[34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n\u001b[32m    437\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m         \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mline\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\programming\\python3.12.10\\Lib\\traceback.py:323\u001b[39m, in \u001b[36mFrameSummary.line\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lineno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    322\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     \u001b[38;5;28mself\u001b[39m._line = \u001b[43mlinecache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlineno\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._line.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\programming\\python3.12.10\\Lib\\linecache.py:30\u001b[39m, in \u001b[36mgetline\u001b[39m\u001b[34m(filename, lineno, module_globals)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetline\u001b[39m(filename, lineno, module_globals=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     27\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a line for a Python source file from the cache.\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m    Update the cache if it doesn't contain an entry for this file already.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     lines = \u001b[43mgetlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_globals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m1\u001b[39m <= lineno <= \u001b[38;5;28mlen\u001b[39m(lines):\n\u001b[32m     32\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m lines[lineno - \u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\programming\\python3.12.10\\Lib\\linecache.py:46\u001b[39m, in \u001b[36mgetlines\u001b[39m\u001b[34m(filename, module_globals)\u001b[39m\n\u001b[32m     43\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m cache[filename][\u001b[32m2\u001b[39m]\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mupdatecache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_globals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mMemoryError\u001b[39;00m:\n\u001b[32m     48\u001b[39m     clearcache()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\programming\\python3.12.10\\Lib\\linecache.py:141\u001b[39m, in \u001b[36mupdatecache\u001b[39m\u001b[34m(filename, module_globals)\u001b[39m\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtokenize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    142\u001b[39m         lines = fp.readlines()\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mSyntaxError\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\programming\\python3.12.10\\Lib\\tokenize.py:457\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen\u001b[39m(filename):\n\u001b[32m    454\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Open a file in read only mode using the encoding detected by\u001b[39;00m\n\u001b[32m    455\u001b[39m \u001b[33;03m    detect_encoding().\u001b[39;00m\n\u001b[32m    456\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m     buffer = \u001b[43m_builtin_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    459\u001b[39m         encoding, lines = detect_encoding(buffer.readline)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# CONFIGURATION\n",
    "INPUT_DIR = \"C:\\programming\\prg\\Devsoc-hackathon\\scraped_data\"  # Directory containing your batch_*.json files\n",
    "OUTPUT_FILE = \"graph_chunks.json\" # Where we save the processed chunks (optional debug)\n",
    "\n",
    "def load_all_data(directory):\n",
    "    \"\"\"Loads all batch JSON files into a single list.\"\"\"\n",
    "    all_pages = []\n",
    "    files = glob.glob(os.path.join(directory, \"batch_*.json\"))\n",
    "    print(f\"üìÇ Loading data from {len(files)} files...\")\n",
    "    \n",
    "    for f_path in files:\n",
    "        try:\n",
    "            with open(f_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                all_pages.extend(data)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error reading {f_path}: {e}\")\n",
    "            \n",
    "    print(f\"‚úÖ Loaded {len(all_pages)} source pages.\")\n",
    "    return all_pages\n",
    "\n",
    "def create_graph_chunks():\n",
    "    # 1. Load Data\n",
    "    raw_pages = load_all_data(INPUT_DIR)\n",
    "    \n",
    "    # 2. Define the Splitter\n",
    "    # We use a smaller chunk size to keep facts precise.\n",
    "    # Overlap is critical to not cut a sentence in half.\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    graph_documents = []\n",
    "    \n",
    "    print(\"üï∏Ô∏è  Generating Graph Chunks...\")\n",
    "    \n",
    "    for page in raw_pages:\n",
    "        # Extract Core Data\n",
    "        title = page.get('title', 'Unknown')\n",
    "        url = page.get('url', 'Unknown')\n",
    "        last_mod = page.get('last_modified', 'Unknown')\n",
    "        \n",
    "        # --- GRAPH EDGE LOGIC ---\n",
    "        # The 'graph_connections' list (from your scraper) is the KEY.\n",
    "        # We must attach these neighbors to *every* chunk of this page.\n",
    "        neighbors = page.get('graph_connections', [])\n",
    "        neighbors_str = \", \".join(neighbors[:50]) # Limit to 50 links to save space\n",
    "        \n",
    "        # Clean Content\n",
    "        content = page.get('content', '')\n",
    "        if not content: continue\n",
    "            \n",
    "        # Split the content\n",
    "        text_chunks = splitter.split_text(content)\n",
    "        \n",
    "        for i, chunk_text in enumerate(text_chunks):\n",
    "            \n",
    "            # --- THE \"GRAPH CHUNK\" MAGIC ---\n",
    "            # We inject the metadata directly into the TEXT so the LLM reads it.\n",
    "            # This allows the LLM to say: \"I see a link to 'Gymkhana' here, let me ask about that.\"\n",
    "            \n",
    "            contextualized_text = f\"\"\"\n",
    "SOURCE_PAGE: {title}\n",
    "LAST_UPDATED: {last_mod}\n",
    "RELATED_TOPICS: {neighbors_str}\n",
    "---------------------\n",
    "{chunk_text}\n",
    "\"\"\"\n",
    "            # Create the Document Object (Standard LangChain format)\n",
    "            # We also keep clean metadata for code-level filtering\n",
    "            doc = Document(\n",
    "                page_content=contextualized_text,\n",
    "                metadata={\n",
    "                    \"source\": url,\n",
    "                    \"title\": title,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"last_modified\": last_mod,\n",
    "                    \"graph_neighbors\": neighbors # Keep the raw list for code logic\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            graph_documents.append(doc)\n",
    "\n",
    "    print(f\"‚úÖ Generated {len(graph_documents)} Graph Chunks.\")\n",
    "    return graph_documents\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    final_chunks = create_graph_chunks()\n",
    "    \n",
    "    # Debug: Print one chunk to see the structure\n",
    "    if final_chunks:\n",
    "        print(\"\\n--- SAMPLE GRAPH CHUNK ---\")\n",
    "        print(final_chunks[0].page_content)\n",
    "        print(\"\\n--- METADATA ---\")\n",
    "        print(final_chunks[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93829d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Loading Model: sentence-transformers/all-mpnet-base-v2 on GPU...\n",
      "üöÄ Starting Knowledge Graph Ingestion...\n",
      "üìÇ Loading data from 24 files...\n",
      "‚úÖ Loaded 3582 source pages.\n",
      "üï∏Ô∏è  Generating Graph Chunks...\n",
      "‚úÖ Generated 9344 Graph Chunks.\n",
      "üß© Prepared 9344 Graph-Enhanced Chunks.\n",
      "üîß Sanitizing metadata for ChromaDB compatibility...\n",
      "üß† Loading Model: sentence-transformers/all-mpnet-base-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 1a5302f7-4a45-48cb-8ce5-6f1924bb38df)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/./config_sentence_transformers.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving to C:/programming/prg/Devsoc-hackathon/chroma_db_graph...\n",
      "   -> Indexing Batch 1/94\n",
      "   -> Indexing Batch 2/94\n",
      "   -> Indexing Batch 3/94\n",
      "   -> Indexing Batch 4/94\n",
      "   -> Indexing Batch 5/94\n",
      "   -> Indexing Batch 6/94\n",
      "   -> Indexing Batch 7/94\n",
      "   -> Indexing Batch 8/94\n",
      "   -> Indexing Batch 9/94\n",
      "   -> Indexing Batch 10/94\n",
      "   -> Indexing Batch 11/94\n",
      "   -> Indexing Batch 12/94\n",
      "   -> Indexing Batch 13/94\n",
      "   -> Indexing Batch 14/94\n",
      "   -> Indexing Batch 15/94\n",
      "   -> Indexing Batch 16/94\n",
      "   -> Indexing Batch 17/94\n",
      "   -> Indexing Batch 18/94\n",
      "   -> Indexing Batch 19/94\n",
      "   -> Indexing Batch 20/94\n",
      "   -> Indexing Batch 21/94\n",
      "   -> Indexing Batch 22/94\n",
      "   -> Indexing Batch 23/94\n",
      "   -> Indexing Batch 24/94\n",
      "   -> Indexing Batch 25/94\n",
      "   -> Indexing Batch 26/94\n",
      "   -> Indexing Batch 27/94\n",
      "   -> Indexing Batch 28/94\n",
      "   -> Indexing Batch 29/94\n",
      "   -> Indexing Batch 30/94\n",
      "   -> Indexing Batch 31/94\n",
      "   -> Indexing Batch 32/94\n",
      "   -> Indexing Batch 33/94\n",
      "   -> Indexing Batch 34/94\n",
      "   -> Indexing Batch 35/94\n",
      "   -> Indexing Batch 36/94\n",
      "   -> Indexing Batch 37/94\n",
      "   -> Indexing Batch 38/94\n",
      "   -> Indexing Batch 39/94\n",
      "   -> Indexing Batch 40/94\n",
      "   -> Indexing Batch 41/94\n",
      "   -> Indexing Batch 42/94\n",
      "   -> Indexing Batch 43/94\n",
      "   -> Indexing Batch 44/94\n",
      "   -> Indexing Batch 45/94\n",
      "   -> Indexing Batch 46/94\n",
      "   -> Indexing Batch 47/94\n",
      "   -> Indexing Batch 48/94\n",
      "   -> Indexing Batch 49/94\n",
      "   -> Indexing Batch 50/94\n",
      "   -> Indexing Batch 51/94\n",
      "   -> Indexing Batch 52/94\n",
      "   -> Indexing Batch 53/94\n",
      "   -> Indexing Batch 54/94\n",
      "   -> Indexing Batch 55/94\n",
      "   -> Indexing Batch 56/94\n",
      "   -> Indexing Batch 57/94\n",
      "   -> Indexing Batch 58/94\n",
      "   -> Indexing Batch 59/94\n",
      "   -> Indexing Batch 60/94\n",
      "   -> Indexing Batch 61/94\n",
      "   -> Indexing Batch 62/94\n",
      "   -> Indexing Batch 63/94\n",
      "   -> Indexing Batch 64/94\n",
      "   -> Indexing Batch 65/94\n",
      "   -> Indexing Batch 66/94\n",
      "   -> Indexing Batch 67/94\n",
      "   -> Indexing Batch 68/94\n",
      "   -> Indexing Batch 69/94\n",
      "   -> Indexing Batch 70/94\n",
      "   -> Indexing Batch 71/94\n",
      "   -> Indexing Batch 72/94\n",
      "   -> Indexing Batch 73/94\n",
      "   -> Indexing Batch 74/94\n",
      "   -> Indexing Batch 75/94\n",
      "   -> Indexing Batch 76/94\n",
      "   -> Indexing Batch 77/94\n",
      "   -> Indexing Batch 78/94\n",
      "   -> Indexing Batch 79/94\n",
      "   -> Indexing Batch 80/94\n",
      "   -> Indexing Batch 81/94\n",
      "   -> Indexing Batch 82/94\n",
      "   -> Indexing Batch 83/94\n",
      "   -> Indexing Batch 84/94\n",
      "   -> Indexing Batch 85/94\n",
      "   -> Indexing Batch 86/94\n",
      "   -> Indexing Batch 87/94\n",
      "   -> Indexing Batch 88/94\n",
      "   -> Indexing Batch 89/94\n",
      "   -> Indexing Batch 90/94\n",
      "   -> Indexing Batch 91/94\n",
      "   -> Indexing Batch 92/94\n",
      "   -> Indexing Batch 93/94\n",
      "   -> Indexing Batch 94/94\n",
      "‚úÖ Knowledge Graph Successfully Built!\n",
      "   You can now query this DB at: C:/programming/prg/Devsoc-hackathon/chroma_db_graph\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# CONFIGURATION\n",
    "DB_DIR = \"C:/programming/prg/Devsoc-hackathon/chroma_db_graph\"  # Separate DB for Graph RAG\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# 2. Force GPU Usage (Crucial for Speed!)\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # MPNet performs better with normalization\n",
    "\n",
    "print(f\"üß† Loading Model: {EMBEDDING_MODEL} on GPU...\")\n",
    "\n",
    "def ingest_knowledge_graph():\n",
    "    # 1. Generate the \"Smart\" Graph Chunks\n",
    "    print(\"üöÄ Starting Knowledge Graph Ingestion...\")\n",
    "    graph_docs = create_graph_chunks()\n",
    "    \n",
    "    if not graph_docs:\n",
    "        print(\"‚ùå No documents found. Check your json files.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üß© Prepared {len(graph_docs)} Graph-Enhanced Chunks.\")\n",
    "\n",
    "    print(\"üîß Sanitizing metadata for ChromaDB compatibility...\")\n",
    "    for doc in graph_docs:\n",
    "        if \"graph_neighbors\" in doc.metadata:\n",
    "            # Convert ['Link A', 'Link B'] -> \"Link A, Link B\"\n",
    "            neighbors = doc.metadata[\"graph_neighbors\"]\n",
    "            if isinstance(neighbors, list):\n",
    "                doc.metadata[\"graph_neighbors\"] = \", \".join(neighbors)\n",
    "            else:\n",
    "                doc.metadata[\"graph_neighbors\"] = str(neighbors)\n",
    "\n",
    "    # 2. Initialize the Embedding Model (The \"Translator\" to Math)\n",
    "    print(f\"üß† Loading Model: {EMBEDDING_MODEL}...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "    # 3. Store in Vector Database (The \"Memory\")\n",
    "    print(f\"üíæ Saving to {DB_DIR}...\")\n",
    "    \n",
    "    # We use batching to ensure we don't crash memory\n",
    "    BATCH_SIZE = 100\n",
    "    total_batches = (len(graph_docs) // BATCH_SIZE) + 1\n",
    "    \n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=DB_DIR, \n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    \n",
    "    for i in range(0, len(graph_docs), BATCH_SIZE):\n",
    "        batch = graph_docs[i : i + BATCH_SIZE]\n",
    "        print(f\"   -> Indexing Batch {i//BATCH_SIZE + 1}/{total_batches}\")\n",
    "        vectorstore.add_documents(batch)\n",
    "        \n",
    "    print(\"‚úÖ Knowledge Graph Successfully Built!\")\n",
    "    print(f\"   You can now query this DB at: {DB_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ingest_knowledge_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5efb7af6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LanguageModelInput' from 'langchain_core.language_models' (c:\\programming\\newenv\\Lib\\site-packages\\langchain_core\\language_models\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_chroma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# CONFIGURATION\u001b[39;00m\n\u001b[32m      5\u001b[39m DB_DIR = \u001b[33m\"\u001b[39m\u001b[33mC:/programming/prg/Devsoc-hackathon/chroma_db_graph\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\programming\\newenv\\Lib\\site-packages\\langchain_huggingface\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_huggingface\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     ChatHuggingFace,  \u001b[38;5;66;03m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[32m      3\u001b[39m )\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_huggingface\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     HuggingFaceEmbeddings,\n\u001b[32m      6\u001b[39m     HuggingFaceEndpointEmbeddings,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_huggingface\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     HuggingFaceEndpoint,\n\u001b[32m     10\u001b[39m     HuggingFacePipeline,\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\programming\\newenv\\Lib\\site-packages\\langchain_huggingface\\chat_models\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_huggingface\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhuggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[32m      2\u001b[39m     TGI_MESSAGE,\n\u001b[32m      3\u001b[39m     TGI_RESPONSE,\n\u001b[32m      4\u001b[39m     ChatHuggingFace,\n\u001b[32m      5\u001b[39m     _convert_dict_to_message,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mTGI_MESSAGE\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTGI_RESPONSE\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mChatHuggingFace\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_convert_dict_to_message\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\programming\\newenv\\Lib\\site-packages\\langchain_huggingface\\chat_models\\huggingface.py:20\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_huggingface\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhuggingface_pipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePipeline\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallbacks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     AsyncCallbackManagerForLLMRun,\n\u001b[32m     18\u001b[39m     CallbackManagerForLLMRun,\n\u001b[32m     19\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlanguage_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     21\u001b[39m     LanguageModelInput,\n\u001b[32m     22\u001b[39m     ModelProfile,\n\u001b[32m     23\u001b[39m     ModelProfileRegistry,\n\u001b[32m     24\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlanguage_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     26\u001b[39m     BaseChatModel,\n\u001b[32m     27\u001b[39m     agenerate_from_stream,\n\u001b[32m     28\u001b[39m     generate_from_stream,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     31\u001b[39m     AIMessage,\n\u001b[32m     32\u001b[39m     AIMessageChunk,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     ToolMessageChunk,\n\u001b[32m     47\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'LanguageModelInput' from 'langchain_core.language_models' (c:\\programming\\newenv\\Lib\\site-packages\\langchain_core\\language_models\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# CONFIGURATION\n",
    "DB_DIR = \"C:/programming/prg/Devsoc-hackathon/chroma_db_graph\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# 1. Initialize\n",
    "print(\"üß† Loading Model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs={'device': 'cuda'}, # Use your RTX 4050\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "vectorstore = Chroma(persist_directory=DB_DIR, embedding_function=embeddings)\n",
    "\n",
    "# 2. Ask a Question\n",
    "query = \"Who are the governors of the Technology Literary Society?\"\n",
    "print(f\"\\nüîé Query: {query}\")\n",
    "\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "# 3. Show Results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(f\"üìÑ Source: {doc.metadata['title']}\")\n",
    "    print(f\"üîó Related: {doc.metadata.get('graph_neighbors', 'None')[:50]}...\")\n",
    "    print(f\"üìù Text Snippet:\\n{doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d91d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Initializing Graph Agent...\n",
      "\n",
      "üöÄ STARTING GRAPH TRAVERSAL: 'What is Gymkhana?'\n",
      "\n",
      "üë£ Step 1: Searching for 'What is Gymkhana?'...\n",
      "   üìÑ Found Node: Constitution of the Technology Students' Gymkhana\n",
      "   ü§î Thought: ANSWER: Technology Students‚Äô Gymkhana is the hub of numerous extra-curricular and co-curricular activities ranging from sports to music, and it is managed by the students.\n",
      "\n",
      "========================================\n",
      "FINAL ANSWER:\n",
      "========================================\n",
      "Technology Students‚Äô Gymkhana is the hub of numerous extra-curricular and co-curricular activities ranging from sports to music, and it is managed by the students.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from time import sleep\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "# 1. Database Path (Must match your ingest script)\n",
    "DB_DIR = \"C:/programming/prg/Devsoc-hackathon/chroma_db_graph\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# 2. LLM Setup (The Reasoning Engine)\n",
    "# Get a free key at gemini/keys\n",
    "MY_GEMINI_KEY= \"AIzaSyBr2OYi0aLoqAXvT_dSsSRch3ixrdYWO_M\" # <--- PASTE YOUR KEY HERE\n",
    "os.environ[\"google_api_key\"] = MY_GEMINI_KEY\n",
    "# ==========================================\n",
    "# CORE LOGIC\n",
    "# ==========================================\n",
    "\n",
    "class GraphRAGAgent:\n",
    "    def __init__(self):\n",
    "        print(\"üß† Initializing Graph Agent...\")\n",
    "        \n",
    "        # 1. Load the Memory (Vector DB)\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=EMBEDDING_MODEL,\n",
    "            model_kwargs={'device': 'cuda'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        self.db = Chroma(persist_directory=DB_DIR, embedding_function=self.embeddings)\n",
    "        \n",
    "        # 2. Load the Brain (LLM)\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            google_api_key=MY_GEMINI_KEY,\n",
    "            model=\"gemini-3-flash-preview\", # Or \"gemini-1.5-flash\" for speed\n",
    "            temperature=0.1 #SEE IF A LITTLE CREATIVITY HELPS\n",
    "        )\n",
    "       \n",
    "        # 3. Define the \"Navigator\" Prompt \n",
    "        # This prompt forces the LLM to decide: Answer vs. I don't know\n",
    "        self.navigator_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are a Graph RAG Agent.\n",
    "        \n",
    "        GOAL: {goal}\n",
    "        \n",
    "        CONTEXT NODE:\n",
    "        --------------------------------------------------\n",
    "        {context}\n",
    "        --------------------------------------------------\n",
    "        \n",
    "        INSTRUCTIONS:\n",
    "        1. Check \"RELATED_TOPICS\" in the text.\n",
    "        2. If the text answers the GOAL, reply ONLY with:\n",
    "           ANSWER: [The answer]\n",
    "        3. If you need to search a related topic, reply ONLY with:\n",
    "           HOP: [Topic Name]\n",
    "           \n",
    "        CONSTRAINT: Do not write any explanations. Start your response strictly with ANSWER or HOP.\n",
    "        If In the end you did not get any answer, reply with: I don't know.\n",
    "        \"\"\")\n",
    "        \n",
    "    def search(self, query):\n",
    "        \"\"\"Standard Retrieval\"\"\"\n",
    "        results = self.db.similarity_search(query, k=1)\n",
    "        if not results:\n",
    "            return None\n",
    "        return results[0] # Return the best chunk\n",
    "\n",
    "    def solve(self, user_query, max_hops=5):\n",
    "        print(f\"\\nüöÄ STARTING GRAPH TRAVERSAL: '{user_query}'\")\n",
    "        current_query = user_query\n",
    "        visited_context = []\n",
    "        \n",
    "        for step in range(max_hops):\n",
    "            sleep(1)\n",
    "            print(f\"\\nüë£ Step {step + 1}: Searching for '{current_query}'...\")\n",
    "            \n",
    "            node = self.search(current_query)\n",
    "            if not node:\n",
    "                print(\"   ‚ùå Dead end. No information found.\")\n",
    "                break\n",
    "                \n",
    "            content = node.page_content\n",
    "            source = node.metadata.get('title', 'Unknown')\n",
    "            # Fix: Ensure we actually get the neighbors if they exist in metadata\n",
    "            neighbors = node.metadata.get('graph_neighbors', '')\n",
    "            \n",
    "            print(f\"   üìÑ Found Node: {source}\")\n",
    "            \n",
    "            # Inject neighbors explicitly into context so LLM sees them clearly\n",
    "            visited_context.append(f\"SOURCE: {source}\\nRELATED_TOPICS: {neighbors}\\nCONTENT: {content}\")\n",
    "            full_context = \"\\n\\n\".join(visited_context)\n",
    "            \n",
    "            chain = self.navigator_prompt | self.llm | StrOutputParser()\n",
    "            try:\n",
    "                decision = chain.invoke({\"goal\": user_query, \"context\": full_context})\n",
    "                print(f\"   ü§î Thought: {decision}\")\n",
    "                \n",
    "                # --- ROBUST PARSING LOGIC ---\n",
    "                # This regex finds \"HOP: ...\" or \"ANSWER: ...\" anywhere in the text\n",
    "                # even if the LLM adds extra fluff.\n",
    "                match = re.search(r\"(ANSWER|HOP):\\s*(.*)\", decision, re.DOTALL)\n",
    "                \n",
    "                if match:\n",
    "                    action = match.group(1) # \"ANSWER\" or \"HOP\"\n",
    "                    value = match.group(2).strip()\n",
    "                    \n",
    "                    if action == \"ANSWER\":\n",
    "                        return value\n",
    "                    elif action == \"HOP\":\n",
    "                        print(f\"   üîó Graph Hop Triggered! Jumping to -> {value}\")\n",
    "                        current_query = value\n",
    "                        continue\n",
    "                else:\n",
    "                    # If LLM failed format, treat it as an answer if it's the last step\n",
    "                    if step == max_hops - 1:\n",
    "                        return decision\n",
    "                    \n",
    "            except Exception as e:\n",
    "                return f\"‚ùå Error during reasoning: {e}\"\n",
    "\n",
    "        return \"‚ùå I ran out of steps (Max Hops Reached).\"\n",
    "\n",
    "# ==========================================\n",
    "# RUN IT\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    agent = GraphRAGAgent()\n",
    "    \n",
    "    # Test Query\n",
    "    # This query requires a \"Hop\": \n",
    "    # It starts at \"Inter-IIT\" -> Finds \"Sports\" -> Finds \"Specific Sport Details\"\n",
    "    response = agent.solve(\"What is Gymkhana?\")\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"FINAL ANSWER:\")\n",
    "    print(\"=\"*40)\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
